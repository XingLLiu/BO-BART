% \subsection{BQ with CART}
% \label{BQ with CART}

% Using Bayesian CART, we will learn a function $f_{CART}$ and approximate:
% \begin{equation}
% 	I_f \approx \int_{x \in \mathcal{X}} f_{\mbox{CART}}(x) p(x) dx
% \label{eq:approx1}
% \end{equation}

% We denote the value of terminal node $\rho_i$ by $f_{\mbox{CART}}(\rho_i)$. 

% We further augment our tree data structure by calculating Eq.~\eqref{eq:integral}, our original integral, for the set of events that reach
% each terminal node:
% \begin{align}
% 	& \int_{x \in \rho_i} f_{\mbox{CART}}(x) p(x) dx \\
% 	&=  f_{\mbox{CART}}(\rho_i) p(\rho_i)
% 	\label{eq:approx2}
% \end{align}

% \textcolor{red}{\textit{Not so sure whether to use $q(\rho_i)$ or $p(\rho_i)$ here. In Eq.~\eqref{eq:integral} we define $p(x)$ as the distribution but I reckon think this $p$ here stands for the terminal probability which I defined as $q$ in Section~\ref{Introduction}.}} \textcolor{blue}{\textit{Let us try to use $p(x)$ throughout; see if you can update Section~\ref{Introduction} accordingly.  Actually, the details of how we calculate / store $p(x)$ don't really belong there. They should be in Section 3.3}}

% Assuming we have $b$ terminal nodes and having Eq.~\eqref{eq:approx2}, Eq.~\eqref{eq:approx1} becomes:
% \begin{align}
% %	& \int_{x \in \mathcal{X}} f(x) p(x) dx \\ 
% %	\int_{x \in \mathcal{X}} f_{\mbox{CART}}(x) p(x) dx \\ 
% 	\sum_{i=1}^b \int_{x \in \rho_i} f_{\mbox{CART}}(x) p(x) dx = \sum_{i=1}^b f_{\mbox{CART}}(\rho_i) p(\rho_i)
% \label{CART sum}
% \end{align}

% Now given a set of trees $T_1, \ldots, T_m$ drawn from the Bayesian CART posterior, we have
% functions $f_{\mbox{CART}}^{(1)}, \ldots, f_{\mbox{CART}}^{(m)}$, each with its own set of terminal
% nodes $\rho_i^{j}$, $i=1, \ldots, b_j$. We use these trees to approximate
% Eq.~\eqref{eq:integral}:
% \begin{align}
% 	I_f = \int f(x)p(x) dx \approx \frac{1}{m} \sum_{j=1}^m \sum_{i=1}^{b_j} f_{\mbox{CART}}^j(\rho_i^j) p(\rho_i^j)
% \end{align}

% After fitting CART, we calculate $I_f$ for each tree drawn from the posterior.
% If we draw $P$ samples, we obtain $I_f^{(1)}, \ldots, I_f^{(P)}$, allowing us to summarise
% the mean and variance as:
% \begin{equation}
% 	\mbox{E}[I_f] = \frac{1}{n} \sum_p I_f^{(p)}
% \end{equation}
% \begin{equation}
% 	\mbox{Var}[I_f] = \frac{1}{n} \sum_p (I_f^{(p)} - \mbox{E}[I_f])^2
% \end{equation}

\subsection{Bayesian Quadrature with BART}
We now present our Bayesian Quadrature with BART method. Let $f$ be the function we wish to integrate over a measure space. We assume that it is H\"{o}lder-continuous for the sake of obtaining nice posterior concentration rates which guarantee that BART will not overfit \cite{rockova2019theory}.
We will learn a function $f_{\bart}$ and approximate Eq.~\eqref{Introduction} with
\begin{equation}
	I_f \approx \int_{\mathbb{R}^d} f_{\bart} d\mu = \int_{\mathbb{R}^d} f_{\bart}(x)p(x)dx.
\label{eq:approx1}
\end{equation}

Since each tree is a step function, there are coefficients for each indicator function that we will call the ``value at the terminal node''. We denote the value of the $i^{th}$ terminal node of the $j^{th}$ posterior draw in the $k^{th}$ tree by $\theta_{i,j}^k$ . Similarly, $p_{i,j}^k = \mu(\Omega_{i,j}^k)$ as discussed before in Eq~(\ref{eq:single tree approximation}). The details to obtain $p_{i,j}^k$ are included in subsection~\ref{p}.

Depending on the tree structure, we have the step functions $f^1_{j,\bart},\ldots,f^K_{j,\bart}$, which stand for each of the $K$ trees, which is the pre-set value in the BART model as the number of trees, in the $j^{th}$ posterior draw. Thus the approximation of Eq.~\eqref{eq:approx1} by the $j^{th}$ set of trees becomes
\begin{align}
	I_f^{(j)} \approx \sum_{k=1}^{K}\sum_{i=1}^{b_{k,j}} \theta_{i,j}^k p_{i,j}^k,
\end{align}
where $b_{k,j}$ is the number of terminal nodes for tree $k$.

After fitting BART, we obtain $N$  trees drawn from the posterior, and calculate $I_f^{(1)}, \ldots, I_f^{(N)}$. These $N$ draws from the posterior over the integral can be summarised using the mean and variance as
\begin{equation}
	\mbox{E}[I_f|\mathcal{D}] \approx \frac{1}{N} \sum_{j=1}^{N} I_f^{(j)},
\end{equation}
\begin{equation}
	\mbox{Var}[I_f|\mathcal{D}] \approx \frac{1}{N} \sum_{j=1}^N \left(I_f^{(j)} - \mbox{E}[I_f|\mathcal{D}]\right)^2.
\end{equation}

\subsection{Calculating Terminal Node Probabilities}
\label{p}

For simplicity, we start by assuming our experiments take place in the unit $d$-dimensional hypercube $[0,1]^d$. We assume a uniform probability measure $\mu$ over the inputs, with constant probability density function $p(x)$. In practice, the measure could instead be Gaussian or discrete. 

For a tree in a posterior draw, the terminal node probability $p$ is obtained through multiplying the probability at leaf node $\theta_{l}$ at level $l$ of the tree along the branch. Suppose we have a $d$-dimensional uniform prior $p(x)$ in range $[0,1]^d$, and $x_r$ is the $r$th element in $x$, given the range $(R_{l,L}^r, R_{l,U}^r)$ of possible $x_r$ being allocated to the node $\theta_l$, and the cutpoint $C_l^r$ on $x_r$, the probability at the next left-branch node making decision on is
\begin{equation}
\label{eq:p(x)}
    p_{l+1,\mbox{Left}} = \frac{C_l^r - R_{l,L}^r}{R_{l,U}^r - R_{l,L}^r}
\end{equation}
and the probability of the next right branch node is
\begin{equation}
    p_{l+1,\mbox{Right}} = 1-p_{l+1,\mbox{Left}} = \frac{R_{l,U}^r - C_l^r}{R_{l,U}^r - R_{l,L}^r}.
\end{equation}

Figure~\ref{fig:tree} gives a brief illustration of how we find the node probability on $d$-dimensional variables.
\begin{center}
\begin{figure}[tbh!]
\scalebox{0.85} {
    \begingroup\catcode`"=9
\begin{tikzpicture}
  [
    grow                    = down,
    sibling distance        = 8em,
    level distance          = 6em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\footnotesize}
  ]
  \node [root] {$(\theta_0, C_0^i, x_i)$}
    child { node [env] {$\theta_{1,\mbox{Left}}=\theta_1$}
      edge from parent node [middle] {$p_{1,\mbox{Left}} = \frac{C_0^i-R_{0,L}^i}{R_{0,U}^i-R_{0,L}^i}$\hspace*{3.1cm}} }
    child { node [env] {$(\theta_{1,\mbox{Right}}, C_1^j, x_j)$}
      child { node [env] {$(\theta_{2,\mbox{Left}}, C_2^k, x^k)$}
        child { node [env] {$\theta_{3,\mbox{Left}}=\theta_2$}
          edge from parent node [middle] {$p_{3,\mbox{Left}} = \frac{C_2^k-R_{2,L}^k}{R_{2,U}^k-R_{2,L}^k}$\hspace*{3.1cm}} }
        child { node [env] {$\theta_{3,\mbox{Right}}=\theta_3$}
                edge from parent node [middle] {\hspace*{3.7cm}$p_{3,\mbox{Right}} = \frac{R_{2,U}^k-C_2^k}{R_{2,U}^k-R_{2,L}^k}$} }
        edge from parent node [middle] {$p_{2,\mbox{Left}} = \frac{C_1^j-R_{1,L}^j}{R_{1,U}^j-R_{1,L}^j}$\hspace*{3.2cm}} }
      child { node [env] {$\theta_{2,\mbox{Right}}=\theta_4$}
              edge from parent node [middle] {\hspace*{3.3cm}$p_{2,\mbox{Right}} = \frac{R_{1,U}^j-C_1^j}{R_{1,U}^j-R_{1,L}^j}$}}
              edge from parent node [middle] {\hspace*{3.3cm}$p_{1,\mbox{Right}} = \frac{R_{0,U}^i-C_0^i}{R_{0,U}^i-R_{0,L}^i}$} };
\end{tikzpicture}
\endgroup
}
\caption{Example of a decision tree with 3 levels for a d-dimensional input and uniform prior. Pairs $(\theta_{s,\mbox{Left}}, C_l^r, x_r)$ and $(\theta_{l,\mbox{Right}}, C_l^r, x_r)$ are the nodes and their associated cutpoints at level $l$ on $x_r$. Terminal node probabilities can be worked out by multiplying along the branches.}
\label{fig:tree}
\vspace*{-0.2in}
\end{figure}
\end{center}
