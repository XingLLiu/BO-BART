% \subsection{BQ with CART}
% \label{BQ with CART}

% Using Bayesian CART, we will learn a function $f_{CART}$ and approximate:
% \begin{equation}
% 	I_f \approx \int_{x \in \mathcal{X}} f_{\mbox{CART}}(x) p(x) dx
% \label{eq:approx1}
% \end{equation}

% We denote the value of terminal node $\rho_i$ by $f_{\mbox{CART}}(\rho_i)$. 

% We further augment our tree data structure by calculating Eq.~\eqref{eq:integral}, our original integral, for the set of events that reach
% each terminal node:
% \begin{align}
% 	& \int_{x \in \rho_i} f_{\mbox{CART}}(x) p(x) dx \\
% 	&=  f_{\mbox{CART}}(\rho_i) p(\rho_i)
% 	\label{eq:approx2}
% \end{align}

% \textcolor{red}{\textit{Not so sure whether to use $q(\rho_i)$ or $p(\rho_i)$ here. In Eq.~\eqref{eq:integral} we define $p(x)$ as the distribution but I reckon think this $p$ here stands for the terminal probability which I defined as $q$ in Section~\ref{Introduction}.}} \textcolor{blue}{\textit{Let us try to use $p(x)$ throughout; see if you can update Section~\ref{Introduction} accordingly.  Actually, the details of how we calculate / store $p(x)$ don't really belong there. They should be in Section 3.3}}

% Assuming we have $b$ terminal nodes and having Eq.~\eqref{eq:approx2}, Eq.~\eqref{eq:approx1} becomes:
% \begin{align}
% %	& \int_{x \in \mathcal{X}} f(x) p(x) dx \\ 
% %	\int_{x \in \mathcal{X}} f_{\mbox{CART}}(x) p(x) dx \\ 
% 	\sum_{i=1}^b \int_{x \in \rho_i} f_{\mbox{CART}}(x) p(x) dx = \sum_{i=1}^b f_{\mbox{CART}}(\rho_i) p(\rho_i)
% \label{CART sum}
% \end{align}

% Now given a set of trees $T_1, \ldots, T_m$ drawn from the Bayesian CART posterior, we have
% functions $f_{\mbox{CART}}^{(1)}, \ldots, f_{\mbox{CART}}^{(m)}$, each with its own set of terminal
% nodes $\rho_i^{j}$, $i=1, \ldots, b_j$. We use these trees to approximate
% Eq.~\eqref{eq:integral}:
% \begin{align}
% 	I_f = \int f(x)p(x) dx \approx \frac{1}{m} \sum_{j=1}^m \sum_{i=1}^{b_j} f_{\mbox{CART}}^j(\rho_i^j) p(\rho_i^j)
% \end{align}

% After fitting CART, we calculate $I_f$ for each tree drawn from the posterior.
% If we draw $P$ samples, we obtain $I_f^{(1)}, \ldots, I_f^{(P)}$, allowing us to summarise
% the mean and variance as:
% \begin{equation}
% 	\mbox{E}[I_f] = \frac{1}{n} \sum_p I_f^{(p)}
% \end{equation}
% \begin{equation}
% 	\mbox{Var}[I_f] = \frac{1}{n} \sum_p (I_f^{(p)} - \mbox{E}[I_f])^2
% \end{equation}

\subsection{BQ with BART}
We now present the procedure used in Bayesian Quadrature with BART, with a querying sequential design.
\label{BQ with BART}
We will learn a function $f_{\mbox{BART}}$ and approximate:
\begin{equation}
	I_f \approx \int_{x \in \mathcal{X}} f_{\mbox{BART}}(x) p(x) dx.
\label{eq:approx1}
\end{equation}
We denote the value of the $i^{th}$ terminal node of the $j^{th}$ posterior draw in the $k^{th}$ tree  $\rho_{i,j}^k$ by $f_{j,\mbox{BART}}^k(\rho_{i,j}^k)$. 

Depending on the tree structure, we have the functions $f^1_{j,\mbox{BART}},\ldots,f^K_{j,\mbox{BART}}$, which stands for each of the $K$ tress, which is the pre-set value in the BART model as the number of trees, in the $j^{th}$ posterior draw. Thus the integral approximation Eq.~\eqref{eq:single tree approximation} given by the $j^{th}$ set of trees becomes
\begin{align}
	I_f^{(j)} \approx \sum_{k=1}^{K}\sum_{i=1}^{b_{k,j}} f^k_{j,\mbox{BART}}(\rho_{i,j}^k) p(\rho_{i,j}^k),
\end{align}
where $b_{k,j}$ is the number of terminal nodes for tree $k$.

After fitting the BART, we calculate $I_f$ for each tree drawn from the posterior.
If we draw $N$ samples, we obtain $I_f^{(1)}, \ldots, I_f^{(N)}$, allowing us to summarise
the mean and variance as:
\begin{equation}
	\mbox{E}[I_f] \approx \frac{1}{N} \sum_{j=1}^{N} I_f^{(j)},
\end{equation}
\begin{equation}
	\mbox{Var}[I_f] \approx \frac{1}{N} \sum_{j=1}^N (I_f^{(j)} - \mbox{E}[I_f])^2.
\end{equation}

\subsection{Probability Associated with Terminal Nodes}
\label{p}

For simplicity, we assume a uniform prior $p(x)$ this will allow us to conveniently reuse the calculations that are already used for sampling. According to \cite{BART}, at each node, all predictors are equally likely to be effective. It is also possible to use other priors, such as a normal prior.

This means that calculating $p(x)$ we mentioned in Section~\ref{Introduction} should
be simple: reuse the calculation that is built into BART for the uniform specification of $p_{\mbox{RULE}}$.

To find the probability, based on the uniform distribution we used, we find the conditional probability on the location of each cutting point in the total interval.
