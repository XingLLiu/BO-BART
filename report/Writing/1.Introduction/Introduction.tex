We assume that we have access to any easy-to-evaluate distribution $p(x)$, a costly-to-evaluate function $f(x)$,
and we wish to approximate the integral:
\begin{equation}
	I_f = \int_{x \in \mathcal{X}} f(x) p(x) dx
\label{eq:integral}
\end{equation}

Having the data set, it is possible to fit a tree model. We augment our tree data structure with the following information: for a given split, there is some probability based on $p(x)$ of following the left path, and $1-p(x)$ of following the right path. We can store these probabilities, so that for every terminal node $\rho_i$, we can calculate $p(\rho_i)$, the probability that an data falls into $\rho_i$, by multiplying the probabilities along the edges of the path by which we reached node $\rho_i$.

Standard quadrature method \cite{BQ} allows us to approximate the integral
\begin{equation}
	I_f = \sum_{i=1}^{n} f(\rho_i)p(\rho_i^j)
\label{eq:single tree approximation}
\end{equation}
where $f(\rho_i)$ is the value of terminal node $\rho_i$ and $n$ is the number of terminal nodes of the tree model.

The BART model, simply speaking, is a sum-of-tree model where each tree is constrained by a regularisation prior to be a weak learner. \cite{BART} When training the model, apart from the data set, we introduce sequential design of generating candidates from the known prior distribution, selecting the best candidates and add to the training set. Our integral $I_f$ could then be approximated by the sum-of-tree form of Eq.~\eqref{eq:single tree approximation}. See Section~\ref{BART} for further explanation.