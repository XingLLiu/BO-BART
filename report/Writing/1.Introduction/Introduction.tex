On a measure space $(\mathbb{R}^d, \mathcal{B}_{\mathbb{R}^d}, \mu)$ with $\mu$ absolutely continuous with respect to the Lebesgue measure, for $d\in\mathbb{N}\backslash \{ 0\}$ and a Borel measurable function $f:\mathbb{R}^d \rightarrow \mathbb{R}$,
we wish to approximate the integral:
\begin{equation}
	I_f =\int_{\mathbb{R}^d} f d\mu = \int_{\mathbb{R}^d} f(x) p(x) dx,
\label{eq:integral}
\end{equation}
where $dx$ represents integration with respect to the Lebesgue measure, and $p$ is the Radon-Nikodym derivative with respect to the Lebesgue measure, by the Radon-Nikodym theorem. This problem could be understood to be applying the integration operator $\mathcal{L}:V \rightarrow \int_{\mathbb{R}^d} (\cdot) d\mu$, where $V$ is a space of  Borel measurable functions.

This problem, Bayesian Quadrature (BQ), generalises classical approaches to numerical integration (e.g.~quadrature) and is at the centre of the emerging field of Probabilistic Numerics (PN). PN takes a Bayesian statistical perspective on numerical methods. The most widely applied method in Probabilistic Numerics is Bayesian Optimisation (BO), in which an unknown objective function $f$ is modelled with a Bayesian statistical model, and maximised using a global optimisation method which critically relies upon the posterior distribution of the function. Gaussian processes  have played the dominant role in Probabilistic Numerics as a convenient Bayesian nonparametric model for $f$ (e.g.~\cite{Rasmussen:2002:BMC:2968618.2968681}). % cite earlier work by O'Hagan and work by Osborne as well 

As with BO, BQ critically relies upon a flexible Bayesian statistical model for the integrand $f$. Although Gaussian processes (GPs) are a powerful non-parametric Bayesian prior for functions $f$ with known posterior consistency results and concentration rates \cite{10.2307/25464673} which carry over to BQ \cite{briol2015frank} and promises faster convergence rates than classical Monte Carlo-based integration. While they are virtually the only choice that has been explored in the BQ literature, in practice they suffer from a number of challenges. GPs are often not very effective in high dimensions \cite{7352306}, evidenced by the fact that in BQ, few applications consider more than about 10 dimensions. While particular kernels can be designed for non-stationarity and categorical variables, both settings nevertheless pose problems. %does the theory known about GPs apply to kernels other than the Gaussian?
Learning kernel hyperparameters is a long-standing challenge in the Gaussian process literature, made even more difficult by the ubiquity of small sample sizes in BQ \cite{Rasmussen:2002:BMC:2968618.2968681}.  Most BQ approaches rely on simple (e.g.~Gaussian) choices for the prior $p(\cdot)$ and kernel (Gaussian again), but neither choice is necessarily appropriate in real settings. Finally, the particular form of the variance of the posterior distribution of a Gaussian process is such that the actual observed values of the function are irrelevant, with a peculiar result---information gained through a sequential, active learning-type setup is not actually incorporated into our model's choices about where to look next (except indirectly through the kernel hyperparameters, which we may retrain.)

As a baseline, classical Monte Carlo integration \cite{Press:2007:NRE:1403886} only requires the ability to sample from $p(\cdot)$, and enjoys solid theoretical justifications \cite{Durrett:2010:PTE:1869916}, but suffers from the problem of having large variance.

BART, a sum-of-trees model extending Bayesian CART, requires little need for hyperparameter tuning, enjoys attractive theoretical results \cite{rockova2017posterior} and is robust, in the sense of not overfitting, to $f$ having unknown regularity \cite{rockova2019theory}. We show how BART is a natural choice for BQ, lending itself to an elegant formulation of Bayesian Quadrature with a simple but effective sequential design approach. 

Viewing a regression tree as a step function , it follows that with a single tree $f(\cdot) = \sum_{i=1}^{n}\theta_j \mathbbm{1}_{\Omega_j}(\cdot)$, where $\{\Omega_j\}_j$ is a set of tree-shape partitions of the tree and \mathbbm{1} is an indicator function \cite{rockova2019theory}, the integral in \eqref{Introduction} becomes 
\begin{equation}
	I_f = \sum_{i=1}^{n} \theta_i p_i,
\label{eq:single tree approximation}
\end{equation}
where $\theta_i$ is the value of terminal node, $n$ is the number of terminal nodes of the tree model and $p_j = \mu(\Omega_j) = \int_{\Omega_j} p(x) dx$, with $dx$ indicating integration with respect to the Lesbesgue measure.

The BART model, simply speaking, is a sum-of-trees model where each tree is constrained by a regularisation prior to be a weak learner \cite{BART}. When training the model, apart from the training set, we introduce a sequential design approach, a form of active learning \cite{doi:10.1002/cjs.11156}, of sampling candidates from a known prior distribution, selecting the best candidates and adding them to the training set. Our integral $I_f$ could then be approximated by the sum-of-tree form of Eq.~\eqref{eq:single tree approximation}. Furthermore, we demonstrate that our sequential design may also be used for quadrature in the context of survey design, where we approximate the moments of stratified and un-stratified population metrics.