We assume that we have access to any easy-to-evaluate distribution $p(x)$, a costly-to-evaluate function $f(x)$,
and we wish to approximate the integral:
\begin{equation}
	I_f = \int_{x \in \mathcal{X}} f(x) p(x) dx,
\label{eq:integral}
\end{equation}

Bayesian Quadrature (BQ) critically relies upon a flexible Bayesian statistical model for the integrand. Although Gaussian processes (GPs) are a powerful nonparametric Bayesian prior for functions that have a number of convenient properties, they are virtually the only choice that has been explored in the BQ literature (see Appendix). 

While GPs have a number of convenient properties, there are a number of settings in which they may not be as effective as BART: high dimensions \cite{7352306}, \textcolor{red}{a mix of real and categorical variables, non-stationary data, and settings which prove very sensitive to difficult to choose kernel hyperparameters}. BART, a sum-of-trees model extending Bayesian CART, requires has little need for hyperparameter tuning and enjoys attractive theoretical results. We show how BART is a natural function choice for BQ, lending itself to an elegant formulation of Bayesian Quadrature with a simple but effective sequential design approach. 


Having the data set, it is possible to fit a tree model. We augment our tree data structure with the following information: for a given split, there is some probability based on $p(x)$ of following the left path, and $1-p(x)$ of following the right path. We can store these probabilities, so that for every terminal node $\rho_i$, we can calculate $p(\rho_i)$, the probability that an data falls into $\rho_i$, by multiplying the probabilities along the edges of the path by which we reached node $\rho_i$.

Standard quadrature method \cite{BQ} allows us to approximate the integral
\begin{equation}
	I_f \approx \sum_{i=1}^{n} f(\rho_i)p(\rho_i),
\label{eq:single tree approximation}
\end{equation}
where $f(\rho_i)$ is the value of terminal node $\rho_i$ and $n$ is the number of terminal nodes of the tree model.

The BART model, simply speaking, is a sum-of-tree model where each tree is constrained by a regularisation prior to be a weak learner \cite{BART}. When training the model, apart from the data set, we introduce sequential design approach, a form of active learning \cite{doi:10.1002/cjs.11156}, of generating candidates from the known prior distribution, selecting the best candidates and adding them to the training set. Our integral $I_f$ could then be approximated by the sum-of-tree form of Eq.~\eqref{eq:single tree approximation}. See Section~\ref{BART} for further explanation.