The major work of BQ with BART is to improve model performance by adding more training data to the original data set.

The way of choosing training point is to randomly generate a set of candidates $x$ from normal distribution, select the most informative one by working out its posterior variance in $f(x)$ weighted by $p(x)$:
\begin{equation}
	x^* = \mbox{argmax}_x \mbox{Var}[f(x)p(x)]
	\label{eq:maxvar}
\end{equation}

For $p(x)$ the uniform distribution, $p(x)$ is some constant $C$ on its support, so assuming $x$ is in the support, we can simplify Eq.~\eqref{eq:maxvar}:
\begin{equation}
	x^* = \mbox{argmax}_{x} \mbox{Var}[f(x)C] = \mbox{argmax}_{x} \mbox{Var}[f(x)]
	\label{eq:maxvar}
\end{equation}

To implement this in practice, we follow the BO with BART approach.

\begin{algorithm}[]
  \caption{Sequential Design}
  \label{alg:SQ}
\begin{algorithmic}
  \STATE {\bfseries Input:} training set X
  \STATE train BART
  \STATE choose $x_1, \ldots, x_n$ from uniform distribution
  \FOR{$i=1$ {\bfseries to} $n$}
  \STATE find $f_{\mbox{BART}}^{(1)}(x_i), \ldots, f_{\mbox{BART}}^{(P)}(x_i)$ 
  \STATE work out the empirical variance
  \ENDFOR
  \STATE $x^* = \mbox{argmax}_{x} \mbox{Var}[f(x)C] = \mbox{argmax}_{x} \mbox{Var}[f(x)]
	\label{eq:maxvar}$
  \STATE $X = (X, x^*)$
  \REPEAT
  \UNTIL{required number of points is reached}
\end{algorithmic}
\end{algorithm}


