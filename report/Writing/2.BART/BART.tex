Given a p-dimensional input $x=(x_{1}, \ldots, x_{p})$ and its response $Y$, BART \cite{BART} aims to make inference on its underlying, unknown function
\begin{equation}
    Y = f(x) + \epsilon\text{, }\epsilon \sim N(0,\sigma^{2})
\label{eq:underlying function}
\end{equation}

Having a Bayesian Classification and Regression Trees (CART) \cite{CART} model $T$ with $b$ terminal nodes and a set of parameters $\Theta = \{\theta_1, \ldots, \theta_b\}$, with $\theta_i$ associated with the $i$th terminal node, denote $g(x; T, \Theta)$ as the function which assigns a $\theta_i \in \Theta$ to \textbf{x}. Thus, a single tree can be expressed as 
\begin{equation}
    Y = g(x; T, \Theta) + \epsilon\text{, }\epsilon \sim N(0, \sigma^2)
\label{eq:single tree}
\end{equation}
Now define the sum-of-trees model as
\begin{equation}
    Y = \sum_{j = 1}^{m}g(x; T_j, \Theta_j) + \epsilon\text{, }\epsilon \sim N(0, \sigma^2)
\label{eq:single tree}
\end{equation}
where m is the number of trees and $g(x; T_j, \Theta_j)$ is the function that assigns $\theta_{ij} \in \Theta_j$ to \textbf{x}.

For a fixed $m$, the sum-of-trees model is determined by $(T_1, \Theta_1), (T_2, \Theta_2), \ldots, (T_m, \Theta_m)$ and $\sigma$.

% \subsubsection{Prior Independence}
% \label{Prior Independence}

% %%%%%%%%%%Mainly copied content%%%%%%%%%%
% Assume independence among tree components $(T_j, \Theta_j)$ and tree's terminal node parameters. The distribution of sum-of-trees model can hence be simplified as:
% \begin{align}
% &P((T_1, \Theta_1), \ldots, (T_m, \Theta_m), \sigma) = [\prod_{j}P(\Theta_j|T_j)P(T_j)]P(\sigma)\\
% &\text{and }P(\Theta_j|T_j) = \prod_{i}P(\theta_{ij}|T_j)
% \label{eq:distribution}
% \end{align}
% where $\theta_{ij}$ is the $i$th terminal node of the $jth$ tree.

% \subsubsection{Parameter Prior}
% \label{Parameter Prior 2}

% %%%%%%%%%%Mainly copied content%%%%%%%%%%
% For parameter prior $P(\mu_{ij}|T_j)$, conjugate normal distribution $N(\mu_{\mu},\sigma_{\mu}^2)$ is used to offer computational benefits. One can conclude that $E(Y|x)$ is the sum of $m$ $\mu_{ij}$â€™s under the sum-of-trees model, and by independence, the induced prior on $E(Y |x)$ is $N(m\mu_{\mu}, m\sigma_{\mu}^2)$.

% Solving the equations 
% \begin{align}
%     &m\mu_{\mu} - k\sqrt{m}\sigma_{\mu} = y_{min}
% \label{eq:find parameters 1}
% \end{align} 
% and 
% \begin{align}
%     m\mu_{\mu} + k\sqrt{m}\sigma_{\mu} = y_{max}
% \label{eq:find parameters 2}
% \end{align}
% for some chosen $k$ allows the distribution to assign substantial probability to the interval $(y_{min}, y_{max})$

% For $P(\sigma)$, we use the inverse chi-square distribution $\sigma^{2} \sim \frac{v\lambda} {\chi_{v}^2}$.

% To find the parameters, one can introduce a chosen quantity $q$, take $\hat{\sigma}$ as the sample standard deviation of $Y$. Then pick up parameter $(v, \lambda)$ such that $P(\sigma < \hat{\sigma}) = q$.

% \subsubsection{Tree Posterior}
% \label{Tree Posterior}

% %%%%%%%%%%Mainly copied content%%%%%%%%%%
% Ideally we want to find the tree posterior by Bayes theorem
% \begin{equation}
%      P(T|X, Y) \propto P(Y|X, T)P(T)
%  \label{eq:T posterior}
%  \end{equation}
% However, it is usually not feasible to evaluate Eq.~\eqref{eq:T posterior} because of the sheer number of possible trees. We instead use the Bayesian Backfitting MCMC algorithm. (See Section~\ref{Backfitting})

% \subsection{Bayesian Backfitting MCMC Algorithm}
% \label{Backfitting}

% %%%%%%%%%%Mainly copied content%%%%%%%%%%
% Let $T_{(j)}$ denote the set of $m-1$ trees which excludes the $j$th tree. Take $m$ successive draws of $(T_j, \Theta_j)$ conditional on $(T_{(j)}, \Theta_{(j)}, \sigma)$
% \begin{equation}
%     (T_j, \Theta_j)|T_{(j)}, \Theta_{(j)}, \sigma, y\text{, }j = 1, \ldots, m
% \label{draw samples}
% \end{equation}
% followed by a draw of $\sigma$ from
% \begin{equation}
%     \sigma|T_1, \ldots, T_m, \Theta_1, \ldots, \Theta_m, y
% \label{draw sigma}
% \end{equation}
% Note that this is an inverse gamma distribution.

% The conditional distribution $P(T_{j},\Theta_{j}|T_{(j)},\Theta_{(j)},\sigma,Y)$ depends on $(T_{(j)},\Theta_{(j)},Y)$ only through the residue 
% \begin{equation}
%     R_j = Y - \sum_{k \neq j}g(x; T_k, \Theta_k)
% \label{eq:residue}
% \end{equation}
% Hence, draws from $(T_j, \Theta_j)|T_{(j)}, \Theta_{(j)}, \sigma, y$
% is equivalent to draws from $(T_j, \Theta_j)|R_j, \sigma$, $j = 1, \ldots, m$.

% Note that $R_j = g(x; T_j, \Theta_j) + \epsilon$ by definition, which is a single tree model in CART. 

% Therefore, we can carry out each draw $(T_j, \Theta_j)$ in two successive steps as $T_j|R_j, \sigma$ and $\Theta_j|T_j, R_j, \sigma$: 
% \begin{enumerate}
%     \item Draw $\sigma$ from the inverse gamma distribution.
%     \item Draw $(T_j, \Theta_j)|R_j, \sigma$ by MH algorithm
%     \item Go back to step 1 with $R_{j + 1}$ and $T_{j + 1}$. 
% \end{enumerate}
Drawing posterior trees by Metropolis-Hastings algorithm \cite{MH} ensures that $(T_1, \Theta_1), \ldots, (T_m, \Theta_m), \sigma$ converges in distribution to $P((T_1, \Theta_{1}), \ldots, (T_m, \Theta_m), \sigma|Y)$ and the induced sum-of-trees function 
\begin{center}
    $f(\cdot) = \sum_{j=1}^{m}g(\cdot; T_j, \Theta_j)$
\end{center}
hence converges to $P(f|Y)$, the posterior distribution of the true $f(\cdot)$.