\documentclass[12pt]{article}
\usepackage{bm}
\newcommand{\x}{\bm{x}}
\usepackage{color}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{graphicx,psfrag,epsf}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{subcaption}
\let\cite\citep
\usepackage{colortbl}
\usepackage{soul}
\usepackage{url}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}


\usepackage{bm}
\newcommand{\bx}{\bm{x}}
\newcommand{\bs}{\bm{s}}

\usepackage[toc,page]{appendix}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\makeatother


\title{Bayesian Optimisation algorithm proposal}
\date{\today}
\author{Seth Flaxman}
\begin{document}
\maketitle 
{\bf Note: this proposal is flawed---it assumes that we are using just one tree, and we have $T$ samples from the posterior.}

Proposed algorithm:

\begin{enumerate}
\item Given an initial dataset $\mathcal{D} = \{x_i,y_i\}_{i=1}^m$, we fit BART to learn a function $f(x) = y$.
\item For each tree $t_1, \ldots, t_T$ we calculate the improvement at each terminal node. We find the maximum improvement 
for each tree $I_1, \ldots, I_T$. This implies a path through the tree, which means a feasibility region in space.
\item We select a point $x_j^*$ at random for which tree $j$ yields this maximum. 
\item We query our black box at locations $x_1^*, \ldots, x_T^*$ to obtain corresponding $y_1^*, \ldots, y_T^*$.
\item We update our dataset $\mathcal{D}$ to include these new observations.
\item We refit BART, using the existing fit as a warm start in our MCMC sampler, and repeat from step 2.
\end{enumerate}

\end{document}
