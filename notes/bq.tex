
\documentclass[12pt]{article}
\usepackage{bm}
\newcommand{\x}{\bm{x}}
\usepackage{color}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{graphicx,psfrag,epsf}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{subcaption}
\let\cite\citep
\usepackage{colortbl}
\usepackage{soul}
\usepackage{url}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}


\usepackage{bm}
\newcommand{\bx}{\bm{x}}
\newcommand{\bs}{\bm{s}}

\usepackage[toc,page]{appendix}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\makeatother


\title{Bayesian Quadrature algorithm proposal}
\date{\today}
\author{Seth Flaxman}
\usepackage[parfill]{parskip}

\begin{document}
\maketitle 

\section{BQ with Bayesian CART (not BART, yet!)}
{\bf This section has been updated as it covers CART, not BART.}

We assume we have access to any easy to evaluate distribution $p(x)$, a costly to evaluate function $f(x)$,
and we wish to approximate the integral:
\begin{equation}
	I_f = \int_{x \in \mathcal{X}} f(x) p(x) dx
\label{eq:integral}
\end{equation}

Using Bayesian CART, we will learn a function $f_{\mbox{BART}}$ and approximate:
\begin{equation}
	I_f \approx \int_{x \in \mathcal{X}} f_{\mbox{BART}}(x) p(x) dx
	\label{eq:approx1}
\end{equation}

We augment our tree data structure with the following information: 
for a given split, there is some probability
based on $p(x)$ of following the left path, and some probability $1-p(x)$ of following the right path. We store
these probabilities, so that for every terminal node $\rho_i$, we can calculate $p(\rho_i)$, the probability of the
event $\rho_i$, by multiplying the probabilities along the edges of the path by which we reached node $\rho_i$.

[INSERT FIGURE ILLUSTRATING THIS HERE]

We denote the value of terminal node $\rho_i$ by $f_{\mbox{CART}}(\rho_i)$.
We further augment our tree data structure by calculating Eq.~\eqref{eq:integral} for the set of events that reach
each terminal node:
\begin{align}
	& \int_{x \in \rho_i} f_{\mbox{CART}}(x) p(x) dx \\
	&=  f_{\mbox{CART}}(\rho_i) p(\rho_i)
	\label{eq:approx2}
\end{align}
Assuming we have $m$ terminal nodes, we use Eq.~\eqref{eq:approx2} to calculate Eq.~\eqref{eq:approx1}:
\begin{align}
%	& \int_{x \in \mathcal{X}} f(x) p(x) dx \\ 
	& \int_{x \in \mathcal{X}} f_{\mbox{CART}}(x) p(x) dx \\ 
	= & \sum_{i=1}^m \int_{x \in \rho_i} f_{\mbox{CART}}(x) p(x) dx \\ 
	= &  \sum_{i=1}^m f_{\mbox{CART}}(\rho_i) p(\rho_i)
\end{align}


Now given a set of trees $t_1, \ldots, t_T$ drawn from the Bayesian CART posterior, we have
functions $f_{\mbox{CART}}^{(1)}, \ldots, f_{\mbox{CART}}^{(T)}$, each with its own set of terminal
nodes $\rho_i^{j}$, $i=1, \ldots, m_j$. We use these trees to approximate
Eq.~\eqref{eq:integral}:
\begin{align}
	I_f = \int f(x)p(x) dx \approx \frac{1}{T} \sum_{j=1}^T \sum_{i=1}^{m_j} f_{\mbox{CART}}^j(\rho_i^j) p(\rho_i^j)
\end{align}

After fitting CART, we calculate $I_f$ for each tree drawn from the posterior.
If we draw $P$ samples, we obtain $I_f^{(1)}, \ldots, I_f^{(P)}$, allowing us to summarize
the mean and variance as:
\begin{equation}
	\mbox{E}[I_f] = \frac{1}{n} \sum_p I_f^{(p)}
\end{equation}
\begin{equation}
	\mbox{Var}[I_f] = \frac{1}{n^2} \sum_p (I_f^{(p)} - \mbox{E}[I_f])^2
\end{equation}

\section{From Bayesian CART to BART}
[TO BE FILLED IN]

\section{Choosing the next query location (Sequential BQ with BART)}
Since it's not at all obvious how to take a Sequential Bayesian Quadrature with GPs approach to the BART setting,
a simple alternative is to look where the posterior variance in $f(x)$ as weighted by $p(x)$ is largest.
Thus, we choose as the next query location:
\begin{equation}
	x^* = \mbox{argmax}_{x} \mbox{Var}[f(x)p(x)]
	\label{eq:maxvar}
\end{equation}
To implement this in practice, we follow the BO with BART approach. Choose a set of test locations $x_1, \ldots, x_n$
using, e.g.~LHS and find the posterior predictve distribution of BART at each $x_i$:
\begin{equation}
	p(f(x_i) | \mathcal{D})
\end{equation}
Since we've used MCMC, what this means in practice is that we obtain a set of $p$ samples for each $x_i$: $f^{(1)}(x_i), \ldots, f^{(p)}(x_i)$ from the $p$ separate sum-of-trees. 
Now we can calculate the variance using the empirical estimator for mean and variance:
\begin{align}
	\mbox{E}[f(x_i)p(x_i)] &= \frac{1}{p} \sum_j f^{(j)}(x_i)p(x_i) \\
	\mbox{Var}[f(x_i)p(x_i)] &= \frac{1}{p} \sum_j \left[f^{(j)}(x_i)p(x_i) - \mbox{E}[f(x_i)p(x_i)]\right]^2
\end{align}
Finally, we find $x^*$ following Eq.~\eqref{eq:maxvar}.

\section{Calculating $p(x)$}
If we assume a uniform prior $p(x)$ this will allow us to conveniently reuse the calculations that are already
used for sampling. In section 3.2 of Chipman, George, and McCulloch [Bayesian CART Model Search] they describe 
$p_{\mbox{RULE}}()\rho | \eta, T)$ and propose what they call the ``uniform specification'' where
at each ndoe, all predictors are equally likely to be effective. This means that calculating $p(x)$ should
be simple: just reuse the calculation that is built into BART for the uniform specification of $p_{\mbox{RULE}}$.

\end{document}
